_BASE_: "./CLIP_fast_rcnn_R_50_C4.yaml"
MODEL:
  MASK_ON: False
  ROI_HEADS:
    NUM_CLASSES: 5
    NMS_THRESH_TEST: 0.6 #Defaults to 0.5 / 0.2
    SCORE_THRESH_TEST: 0.001 #Defaults to 0.001 / 0.1
    SOFT_NMS_ENABLED: False #Defaults to False
  CLIP:
    NO_BOX_DELTA: True #set to True only detecting OOD class or for zero-shot (model aren't trained on those class), set false improves detection on base classes (box delta realigns prediction coordinates based on learned class head)
    OFFLINE_RPN_NMS_THRESH: 0.7 #default 0.9 for custom or zsinf
    # MULTIPLY_RPN_SCORE: True #uncomment for >100% scores with objectiveness logits
    VIS: True # Note: visualize the scores before multiplying RPN scores, if any
    #BG_CLS_SCORE: False #generate all rpn bbox and scores include background bboxes and scores
    OFFLINE_RPN_PRE_NMS_TOPK_TEST: 12000 #default 6000
    OFFLINE_RPN_POST_NMS_TOPK_TEST: 2000 #default 1000
    CLS_ID_NMS: True #default False
    CROP_REGION_TYPE: "RPN" #default "RPN" can switch to GT (clip style evaluation)
DATASETS:
#  TRAIN: ("lvis_v1_train_custom_img",) #not needed for --eval-only
#  TEST: ("lvis_v1_val_custom_img",)  #custom images  (lvis_instances_results.json)
  TRAIN: ("humanware_train_basic",)
#  TEST: ("humanware_test_custom",)
#  TEST: ("humanware_test_basic_63",)
#  TEST: ("humanware_test_basic",)
  TEST: ("humanware_test_collected",)
#  TEST: ("humanware_test_full",)
#  TEST: ("humanware_test_awake",)
INPUT:
  MIN_SIZE_TEST: 800
  MAX_SIZE_TEST: 1333
TEST:
  #lvis allows up to 300 detections per image
  DETECTIONS_PER_IMAGE: 300 #default 100, set equal POST_NMS_TOPK_TEST to detect all from RPN
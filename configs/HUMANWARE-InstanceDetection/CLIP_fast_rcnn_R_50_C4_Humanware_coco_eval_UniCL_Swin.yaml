_BASE_: "./CLIP_fast_rcnn_R_50_C4_ovd.yaml"
MODEL:
  BACKBONE:
    NAME: "build_clip_swin_backbone"
    FREEZE_AT: 2
  SWIN:
    OUT_FEATURES: ["stage5"]
    #default config for UniCL
    EMBED_DIM: 128
    DEPTHS: [ 2, 2, 18, 2 ]
    NUM_HEADS: [ 4, 8, 16, 32 ]
    DROP_PATH_RATE: 0.5
    WINDOW_SIZE: 7
  # TEXT:
  #   AUTOGRESSIVE: true #default true 
  #   #TODO language encoder defaults 
  #   #add in detectron2/modeling/backbone/clip_lang_encoder.py line 146
  #   #for transformer
    # WIDTH: 512
    # HEADS: 8
    # LAYERS: 12  

  ROI_HEADS:
    NAME: "CLIPTransformerROIHeads"
    IN_FEATURES: ["stage5"]
    NUM_CLASSES: 5
    NMS_THRESH_TEST: 0.2 #Defaults to 0.5
    SCORE_THRESH_TEST: 0.1 #Defaults to 0.001
    SOFT_NMS_ENABLED: False #Defaults to False
  CLIP:
    # NO_BOX_DELTA: False #default false set to True only detecting OOD class or for zero-shot (model aren't trained on those class), set false improves detection on base classes (box delta realigns prediction coordinates based on learned class head)
    OFFLINE_RPN_NMS_THRESH: 0.7 #default 0.7
    #BG_CLS_SCORE: False #include background bboxes and scores
    OFFLINE_RPN_PRE_NMS_TOPK_TEST: 12000 #default 6000
    OFFLINE_RPN_POST_NMS_TOPK_TEST: 2000 #default 1000
    CLS_ID_NMS: True #default False
    CROP_REGION_TYPE: "RPN" #default "RPN" can switch to GT (clip style evaluation)
DATASETS:
  TRAIN: ("humanware_train_basic",)
  TEST: ("humanware_test_custom",)
#  TEST: ("humanware_test_basic",)
#  TEST: ("humanware_test_collected",)
#  TEST: ("humanware_test_full",)
#  TEST: ("humanware_test_awake",)
INPUT:
  MIN_SIZE_TEST: 800
  MAX_SIZE_TEST: 1333
TEST:
  DETECTIONS_PER_IMAGE: 1000 #default 100 set equal POST_NMS_TOPK_TEST to detect all from RPN